{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import findspark\n",
    "#useful to install this tool to simplify spark import\n",
    "findspark.init()\n",
    "from pyspark import  SparkContext\n",
    "sc = SparkContext( 'local[*]', 'pyspark')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top=\"33173 33475 1076 35300 15743\"\n",
    "\n",
    "def test_item_filter(x):\n",
    "    for i in x[1]:\n",
    "        if i[0] in test_users:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def map_string (x):\n",
    "        y=x[1]\n",
    "        s=\"\"\n",
    "        for i in y:\n",
    "            s+=str(i[0])+\" \"\n",
    "        z=len(y)\n",
    "        top_rates=[33173, 33475, 1076, 35300, 15743]\n",
    "        i=0\n",
    "        while (z<5):\n",
    "            s+=str(top_rates[i])\n",
    "            i+=1\n",
    "            z+=1\n",
    "        if (y[0][1]<8):\n",
    "            s=top\n",
    "        return x[0],s\n",
    "\n",
    "#feature1, feature2: [[feature_1],[feature_2],...[feature_N]]\n",
    "def jaccard_sim(feature1,feature2):\n",
    "    s1=set([feat[0] for feat in feature1])\n",
    "    s2=set([feat[0] for feat in feature2])\n",
    "    if len(s1 | s2)==0: return 0\n",
    "    return float(len(s1 & s2)) / len(s1 | s2)\n",
    "\n",
    "feat_rdd=sc.textFile('icm_no_header.csv')\n",
    "train_rdd = sc.textFile(\"train_no_header.csv\")\n",
    "test_rdd = sc.textFile(\"test_no_header.csv\")\n",
    "test_users=test_rdd.collect()[0:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'35236', [[u'1573'], [u'5786'], [u'5787'], [u'9029'], [u'14755']]),\n",
       " (u'35540', [[u'18904']]),\n",
       " (u'35542', [[u'268'], [u'17043'], [u'17181'], [u'17805'], [u'18904']]),\n",
       " (u'3382', [[u'566'], [u'2277'], [u'9399'], [u'17952'], [u'18904']]),\n",
       " (u'35544',\n",
       "  [[u'2403'], [u'3248'], [u'9003'], [u'9004'], [u'12851'], [u'19584']]),\n",
       " (u'13357',\n",
       "  [[u'2068'],\n",
       "   [u'2275'],\n",
       "   [u'4557'],\n",
       "   [u'5787'],\n",
       "   [u'10869'],\n",
       "   [u'15634'],\n",
       "   [u'18904']]),\n",
       " (u'35546',\n",
       "  [[u'3332'],\n",
       "   [u'5787'],\n",
       "   [u'9004'],\n",
       "   [u'9727'],\n",
       "   [u'12362'],\n",
       "   [u'12543'],\n",
       "   [u'18549']]),\n",
       " (u'35548',\n",
       "  [[u'1536'], [u'5787'], [u'11455'], [u'15634'], [u'16260'], [u'16602']]),\n",
       " (u'24029',\n",
       "  [[u'1829'], [u'4820'], [u'8922'], [u'12701'], [u'13169'], [u'15634']]),\n",
       " (u'32239', [[u'4804'], [u'5786'], [u'10801'], [u'13846']])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_rdd=feat_rdd.map(lambda x: x.split(',')).map(lambda x: (x[0],[x[1]]))\n",
    "\n",
    "test_item_votes_rdd=(train_rdd\n",
    "                     .map(lambda x: x.split(','))\n",
    "                     .map(lambda x:(x[1],[x[0],int(x[2])]))\n",
    "                     .groupByKey()\n",
    "                     .filter(test_item_filter)\n",
    "                     .map(lambda x: (x[0],list(x[1]))))\n",
    "\n",
    "test_items=test_item_votes_rdd.map(lambda x:x[0]).collect()\n",
    "item_feat_rdd=feat_rdd.groupByKey()\n",
    "item_feat_rdd.map(lambda x:(x[0],list(x[1]))).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4244',\n",
       "  [[u'3828'], [u'4346'], [u'8922'], [u'9122'], [u'15200'], [u'18904']]),\n",
       " (u'20252', [[u'9879'], [u'10708'], [u'18463'], [u'18904']]),\n",
       " (u'29260',\n",
       "  [[u'515'],\n",
       "   [u'4638'],\n",
       "   [u'7482'],\n",
       "   [u'9003'],\n",
       "   [u'9004'],\n",
       "   [u'10444'],\n",
       "   [u'12362']]),\n",
       " (u'11012', [[u'5787'], [u'12284'], [u'14260'], [u'17986']]),\n",
       " (u'33755',\n",
       "  [[u'3232'],\n",
       "   [u'4601'],\n",
       "   [u'10779'],\n",
       "   [u'12362'],\n",
       "   [u'15634'],\n",
       "   [u'16920'],\n",
       "   [u'18904']]),\n",
       " (u'5861', [[u'621'], [u'2405'], [u'4715'], [u'7577'], [u'12284']]),\n",
       " (u'19036',\n",
       "  [[u'303'], [u'3293'], [u'5470'], [u'10273'], [u'15634'], [u'18904']]),\n",
       " (u'32956',\n",
       "  [[u'2483'], [u'4913'], [u'5787'], [u'6262'], [u'12361'], [u'14848']]),\n",
       " (u'13649',\n",
       "  [[u'5786'],\n",
       "   [u'6215'],\n",
       "   [u'7412'],\n",
       "   [u'10081'],\n",
       "   [u'12362'],\n",
       "   [u'15268'],\n",
       "   [u'18904']]),\n",
       " (u'17924', [[u'845'], [u'5309'], [u'5960'], [u'18904']])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only the features for the item voted by the test users\n",
    "test_item_feat_rdd=item_feat_rdd.filter(lambda x: x[0] in test_items)\n",
    "test_item_feat_rdd.map(lambda x:(x[0], list(x[1]))).take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e372d0>),\n",
       "  (u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e37450>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e37c10>),\n",
       "  (u'35540', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cd10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e37c10>),\n",
       "  (u'35542', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c910>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c810>),\n",
       "  (u'3382', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cc50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c810>),\n",
       "  (u'35544', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cfd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c810>),\n",
       "  (u'13357', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c8d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c810>),\n",
       "  (u'35546', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c950>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'35548', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cad0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'24029', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'32239', <pyspark.resultiterable.ResultIterable at 0x7fef35e7ca10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'35238', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cb50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'11542', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c7d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'13359', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c9d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'11540', <pyspark.resultiterable.ResultIterable at 0x7fef35e7ce90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf50>),\n",
       "  (u'11546', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c790>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'12763', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c590>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'11544', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c610>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'14545', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c710>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'11548', <pyspark.resultiterable.ResultIterable at 0x7fef35e7ca90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'28999', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cc10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'5988', <pyspark.resultiterable.ResultIterable at 0x7fef35f16050>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'35230', <pyspark.resultiterable.ResultIterable at 0x7fef35f16e90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'19399', <pyspark.resultiterable.ResultIterable at 0x7fef35f16550>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'5982', <pyspark.resultiterable.ResultIterable at 0x7fef35f16c50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'19395', <pyspark.resultiterable.ResultIterable at 0x7fef35f16650>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'19393', <pyspark.resultiterable.ResultIterable at 0x7fef35f16f90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'19391', <pyspark.resultiterable.ResultIterable at 0x7fef35f16dd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'22261', <pyspark.resultiterable.ResultIterable at 0x7fef35f16990>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'32233', <pyspark.resultiterable.ResultIterable at 0x7fef35f16350>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'22263', <pyspark.resultiterable.ResultIterable at 0x7fef35f16f10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c650>),\n",
       "  (u'22265', <pyspark.resultiterable.ResultIterable at 0x7fef35f16310>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'9485', <pyspark.resultiterable.ResultIterable at 0x7fef35f16950>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'22267', <pyspark.resultiterable.ResultIterable at 0x7fef35f162d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'22269', <pyspark.resultiterable.ResultIterable at 0x7fef35f16c90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25507', <pyspark.resultiterable.ResultIterable at 0x7fef35f16bd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'32231', <pyspark.resultiterable.ResultIterable at 0x7fef35f16a50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'3380', <pyspark.resultiterable.ResultIterable at 0x7fef35f16590>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25062', <pyspark.resultiterable.ResultIterable at 0x7fef35f16910>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25060', <pyspark.resultiterable.ResultIterable at 0x7fef35f16a90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25066', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25064', <pyspark.resultiterable.ResultIterable at 0x7fef35f16a10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'6970', <pyspark.resultiterable.ResultIterable at 0x7fef35f16b90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'32237', <pyspark.resultiterable.ResultIterable at 0x7fef35f16b10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'25068', <pyspark.resultiterable.ResultIterable at 0x7fef35f16b50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'12761', <pyspark.resultiterable.ResultIterable at 0x7fef35f169d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'13548', <pyspark.resultiterable.ResultIterable at 0x7fef35f16e10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'32235', <pyspark.resultiterable.ResultIterable at 0x7fef35f160d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'960', <pyspark.resultiterable.ResultIterable at 0x7fef35f16110>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'30646', <pyspark.resultiterable.ResultIterable at 0x7fef4200bcd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'27370', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c5d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'28995', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cdd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'27372', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cd50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'27374', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cb10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'962', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cd90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'27376', <pyspark.resultiterable.ResultIterable at 0x7fef35e7cf10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'21091', <pyspark.resultiterable.ResultIterable at 0x7fef35e7ca50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'16642', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c850>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'28997', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c6d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'6798', <pyspark.resultiterable.ResultIterable at 0x7fef35e7ce10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'35126', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c550>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'36255', <pyspark.resultiterable.ResultIterable at 0x7fef35e7c990>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'28991', <pyspark.resultiterable.ResultIterable at 0x7fef35e80750>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f16d10>),\n",
       "  (u'36257', <pyspark.resultiterable.ResultIterable at 0x7fef35e80050>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'21077', <pyspark.resultiterable.ResultIterable at 0x7fef35e80b10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'15959', <pyspark.resultiterable.ResultIterable at 0x7fef35e80210>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'30765', <pyspark.resultiterable.ResultIterable at 0x7fef35e80810>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'28993', <pyspark.resultiterable.ResultIterable at 0x7fef35e80190>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'25288', <pyspark.resultiterable.ResultIterable at 0x7fef35e80090>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'25286', <pyspark.resultiterable.ResultIterable at 0x7fef35e80890>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'36251', <pyspark.resultiterable.ResultIterable at 0x7fef35e80650>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'25284', <pyspark.resultiterable.ResultIterable at 0x7fef35e80550>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'25282', <pyspark.resultiterable.ResultIterable at 0x7fef35e80110>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'25280', <pyspark.resultiterable.ResultIterable at 0x7fef35e803d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'19973', <pyspark.resultiterable.ResultIterable at 0x7fef35e809d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'10691', <pyspark.resultiterable.ResultIterable at 0x7fef35e80d90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'19971', <pyspark.resultiterable.ResultIterable at 0x7fef35e80450>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'15368', <pyspark.resultiterable.ResultIterable at 0x7fef35e80410>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'19977', <pyspark.resultiterable.ResultIterable at 0x7fef35e80ad0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'36253', <pyspark.resultiterable.ResultIterable at 0x7fef35e80990>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'19975', <pyspark.resultiterable.ResultIterable at 0x7fef35e80a10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'278', <pyspark.resultiterable.ResultIterable at 0x7fef35e800d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'16707', <pyspark.resultiterable.ResultIterable at 0x7fef35e80910>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'16701', <pyspark.resultiterable.ResultIterable at 0x7fef35e85e90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'16703', <pyspark.resultiterable.ResultIterable at 0x7fef35e85850>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'22485', <pyspark.resultiterable.ResultIterable at 0x7fef35e85f90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'4646', <pyspark.resultiterable.ResultIterable at 0x7fef35e85d90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'22487', <pyspark.resultiterable.ResultIterable at 0x7fef35e85f10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'12019', <pyspark.resultiterable.ResultIterable at 0x7fef35e85ed0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'22483', <pyspark.resultiterable.ResultIterable at 0x7fef35e85dd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'12015', <pyspark.resultiterable.ResultIterable at 0x7fef35e85a50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'12017', <pyspark.resultiterable.ResultIterable at 0x7fef35e85bd0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'12011', <pyspark.resultiterable.ResultIterable at 0x7fef35e85810>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'12013', <pyspark.resultiterable.ResultIterable at 0x7fef35e85ad0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'17258', <pyspark.resultiterable.ResultIterable at 0x7fef35e85b90>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'30701', <pyspark.resultiterable.ResultIterable at 0x7fef35e85990>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'26397', <pyspark.resultiterable.ResultIterable at 0x7fef35e859d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'4431', <pyspark.resultiterable.ResultIterable at 0x7fef35e85a10>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'26391', <pyspark.resultiterable.ResultIterable at 0x7fef35e85b50>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'26393', <pyspark.resultiterable.ResultIterable at 0x7fef35e858d0>)),\n",
       " ((u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e80290>),\n",
       "  (u'1371', <pyspark.resultiterable.ResultIterable at 0x7fef35e85b10>))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here the order in which we consider the cartesian product matters. Similarity of item1,item2 is equal to \n",
    "#similarity of item2,item1 but we must consider both the case in which the user has voted item1 and \n",
    "# the case in which user has voted item2. So, the cartesian is a total cartesian\n",
    "item_feat_rdd_cart1 = item_feat_rdd.cartesian(item_feat_rdd)\n",
    "item_feat_rdd_cart2 = item_feat_rdd_cart1.map(lambda x: (x[1],x[0]))\n",
    "\n",
    "item_feat_rdd_cart = item_feat_rdd_cart1.union(item_feat_rdd_cart2)\n",
    "item_feat_rdd_cart.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'20252', u'29260', u'11012', u'19036', u'5861', u'14976', u'17924', u'23431', u'12750', u'3100', u'14716', u'11366', u'20959', u'10028', u'26454', u'36855', u'5681', u'36204', u'27604', u'13649', u'19841', u'3674', u'33755', u'19188', u'25912', u'4244', u'15917', u'27648', u'10820', u'22513', u'17782', u'9607', u'24063', u'23217', u'32019', u'32956', u'14629', u'16397', u'24479', u'10766', u'14669', u'4911', u'24440', u'19907', u'20428', u'30979', u'11042', u'14322', u'17107', u'17815', u'25656', u'14878', u'17701', u'3981', u'33213', u'24514', u'2164', u'31444', u'9855', u'16511']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35e37310>),\n",
       "  (u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35e371d0>)),\n",
       " ((u'20252', <pyspark.resultiterable.ResultIterable at 0x7fef35e85390>),\n",
       "  (u'35236', <pyspark.resultiterable.ResultIterable at 0x7fef35f0bed0>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35f0bf50>),\n",
       "  (u'35540', <pyspark.resultiterable.ResultIterable at 0x7fef35f0be50>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35f0b150>),\n",
       "  (u'35542', <pyspark.resultiterable.ResultIterable at 0x7fef35f0b190>)),\n",
       " ((u'20252', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4e50>),\n",
       "  (u'35540', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4f10>)),\n",
       " ((u'20252', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4e50>),\n",
       "  (u'35542', <pyspark.resultiterable.ResultIterable at 0x7fef35f0b190>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4ad0>),\n",
       "  (u'3382', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4bd0>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4fd0>),\n",
       "  (u'35544', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4b50>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4fd0>),\n",
       "  (u'13357', <pyspark.resultiterable.ResultIterable at 0x7fef36954e10>)),\n",
       " ((u'4244', <pyspark.resultiterable.ResultIterable at 0x7fef35ef4fd0>),\n",
       "  (u'35546', <pyspark.resultiterable.ResultIterable at 0x7fef36954990>))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only the tests users\n",
    "test_item_feat_rdd_cart = (item_feat_rdd_cart\n",
    "                           .filter(lambda x: (x[0][0] in test_items and x[1][0] not in test_items)))\n",
    "\n",
    "print test_items\n",
    "test_item_feat_rdd_cart.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute similarity between items\n",
    "test_item_item_sim = (test_item_feat_rdd_cart\n",
    "                      .map(lambda x: ([x[0][0],x[1][0]],jaccard_sim(x[0][1],x[1][1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([u'4244', u'35236'], 0.0),\n",
       " ([u'20252', u'35236'], 0.0),\n",
       " ([u'4244', u'35540'], 0.16666666666666666),\n",
       " ([u'4244', u'35542'], 0.1),\n",
       " ([u'20252', u'35540'], 0.25),\n",
       " ([u'20252', u'35542'], 0.125),\n",
       " ([u'4244', u'3382'], 0.1),\n",
       " ([u'4244', u'35544'], 0.0),\n",
       " ([u'4244', u'13357'], 0.08333333333333333),\n",
       " ([u'4244', u'35546'], 0.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_item_item_sim.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', [u'2738', 1]),\n",
       " (u'1', [u'4716', 4]),\n",
       " (u'1', [u'13298', 8]),\n",
       " (u'1', [u'15122', 4]),\n",
       " (u'2', [u'11326', 5]),\n",
       " (u'3', [u'3406', 4]),\n",
       " (u'3', [u'3906', 10]),\n",
       " (u'3', [u'6345', 6]),\n",
       " (u'3', [u'7393', 8]),\n",
       " (u'4', [u'815', 8])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shaping for join\n",
    "\n",
    "test_item_votes_rdd = test_item_votes_rdd.map(lambda x: ((),[x[0],x[1][1]]))\n",
    "\n",
    "\n",
    "\n",
    "test_item_user_vote=(train_rdd\n",
    "                     .map(lambda x: x.split(','))\n",
    "                     .map(lambda x:(x[1],[x[0],int(x[2])])))\n",
    "                     #.groupByKey()\n",
    "                     #.map(lambda x: (x[0], list(x[1]))))\n",
    "test_item_user_vote.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'4244', [u'35236', 0.0]),\n",
       " (u'20252', [u'35236', 0.0]),\n",
       " (u'4244', [u'35540', 0.16666666666666666]),\n",
       " (u'4244', [u'35542', 0.1]),\n",
       " (u'20252', [u'35540', 0.25]),\n",
       " (u'20252', [u'35542', 0.125]),\n",
       " (u'4244', [u'3382', 0.1]),\n",
       " (u'4244', [u'35544', 0.0]),\n",
       " (u'4244', [u'13357', 0.08333333333333333]),\n",
       " (u'4244', [u'35546', 0.0])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shaping for join\n",
    "test_item_item_sim = test_item_item_sim.map(lambda x: (x[0][0],[x[0][1],x[1]]))\n",
    "test_item_item_sim.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-3bf306b3ef1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mjoined_item_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_item_user_vote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_item_item_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjoined_item_sim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "joined_item_sim = test_item_user_vote.join(test_item_item_sim)\n",
    "joined_item_sim.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 144.0 failed 1 times, most recent failure: Lost task 3.0 in stage 144.0 (TID 153, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1868, in combine\n    merger.mergeValues(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-76-980fb9df7f27>\", line 5, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1868, in combine\n    merger.mergeValues(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-76-980fb9df7f27>\", line 5, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-980fb9df7f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m user_item_rec = (joined_item_sim.map(lambda x: (x[1][0],[x[2][0],compute_rec(x[1],x[2])]))\n\u001b[1;32m----> 6\u001b[1;33m                 .groupByKey()).take(10)\n\u001b[0m\u001b[0;32m      7\u001b[0m                 \u001b[1;31m#.map(lambda x: (x[0],sorted(list(y for y in x[1]),key=lambda x: -x[1]))))#.take(10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 144.0 failed 1 times, most recent failure: Lost task 3.0 in stage 144.0 (TID 153, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1868, in combine\n    merger.mergeValues(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-76-980fb9df7f27>\", line 5, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1868, in combine\n    merger.mergeValues(iterator)\n  File \"/home/nico/spark-1.5.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-76-980fb9df7f27>\", line 5, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def compute_rec(joined_item_sim_vote, item_sim):\n",
    "    k=1   #change this in order to change the weight of a vote\n",
    "    return user_vote[1]*item_sim[1]*k\n",
    "\n",
    "user_item_rec = (joined_item_sim.map(lambda x: (x[1][0],[x[2][0],compute_rec(x[1],x[2])]))\n",
    "                .groupByKey())\n",
    "                #.map(lambda x: (x[0],sorted(list(y for y in x[1]),key=lambda x: -x[1]))))#.take(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
